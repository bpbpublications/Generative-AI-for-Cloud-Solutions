{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain_experimental.comprehend_moderation.base_moderation_exceptions import ModerationPiiError, ModerationPromptSafetyError, ModerationToxicityError\n",
    "from langchain_experimental.comprehend_moderation import BaseModerationConfig, ModerationPromptSafetyConfig, ModerationPiiConfig, ModerationToxicityConfig\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock_text_generation_model(query, modelId):\n",
    "    ## Enter query to bedrock to generate a response from query and context\n",
    "    anthropic_model_kwargs = { \n",
    "        \"max_tokens_to_sample\": 1024,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"stop_sequences\": [\"Human:\"]\n",
    "    }\n",
    "    llm = Bedrock(\n",
    "        model_id=modelId,\n",
    "        model_kwargs=anthropic_model_kwargs)\n",
    "\n",
    "    prompt = PromptTemplate(input_variables=[\"query\"], template=(\"Respond concisely: {query}\"))\n",
    "\n",
    "    pii_config = ModerationPiiConfig(labels=[\"SSN\"], redact=True, mask_character=\"X\")\n",
    "    toxicity_config = ModerationToxicityConfig(threshold=0.6)\n",
    "    prompt_safety_config = ModerationPromptSafetyConfig(threshold=0.8)\n",
    "    moderation_config = BaseModerationConfig(filters=[ toxicity_config, \n",
    "        pii_config, \n",
    "        prompt_safety_config])\n",
    "    comprehend_moderation = AmazonComprehendModerationChain(moderation_config=moderation_config,\n",
    "        client=crclient,\n",
    "        verbose=False,  # optional\n",
    "    )\n",
    "\n",
    "    chain = (\n",
    "        prompt\n",
    "        | {\"input\": (lambda x: x.text)}\n",
    "        | comprehend_moderation\n",
    "        | {\"input\": (lambda x: x[\"output\"]) | llm}\n",
    "        | comprehend_moderation\n",
    "    )\n",
    "    #chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    try:\n",
    "        llm_result = chain.invoke({\"query\": query})\n",
    "        return llm_result['output']\n",
    "    except ModerationPiiError as mpe:\n",
    "        return str(mpe)\n",
    "    except ModerationPromptSafetyError as mse:\n",
    "        return str(mse)\n",
    "    except ModerationToxicityError as mte:\n",
    "        return str(mte)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chain: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Unknown error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_text_generation_model = 'anthropic.claude-v2:1'\n",
    "crclient = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The prompt is unsafe and cannot be processed'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_bedrock_text_generation_model(\"Tell me a joke about people I hate\", bedrock_text_generation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Here is concise Python code to compute a fast Fourier transform:\\n\\n```python\\nimport numpy as np\\nfrom numpy.fft import fft\\n\\nsignal = np.array([1, 2, 3, 4, 5])\\nfourier = fft(signal)\\n```\\n\\nThis imports numpy and the FFT function from numpy. It creates a sample signal as a numpy array, then computes the FFT by calling fft() on the signal. The result is stored in the variable fourier.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_bedrock_text_generation_model(\"Write Python code to compute a fast fourier transform\", bedrock_text_generation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
